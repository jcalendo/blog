[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Bootstrapping Differential Expression\n\n\n\n\n\n\n\nR\n\n\nbioinformatics\n\n\n\n\nHow many can I do?\n\n\n\n\n\n\nSep 22, 2023\n\n\nGennaro Calendo\n\n\n\n\n\n\n  \n\n\n\n\nJoining Millions of Strings in R\n\n\n\n\n\n\n\nR\n\n\n\n\nIs there a fast way to join millions of string in R?\n\n\n\n\n\n\nJul 22, 2023\n\n\nGennaro Calendo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/joining-strings/index.html",
    "href": "posts/joining-strings/index.html",
    "title": "Joining Millions of Strings in R",
    "section": "",
    "text": "I recently had to concatenate tens of millions of strings into a single column of strings. I was surprised when even my data.table code, which was something like dt[, Location := paste(chr, start, sep = \"-\")], was taking minutes.\nSo then, what is the fastest way to combine a bunch of strings?\nSince I really care about this in the context of genetic data I’ll simulate strings from “Chromosomes” and “Start positions” and concatenate them into a single “Loci”. e.g. “chr1-45678”"
  },
  {
    "objectID": "posts/joining-strings/index.html#the-candidates",
    "href": "posts/joining-strings/index.html#the-candidates",
    "title": "Joining Millions of Strings in R",
    "section": "The Candidates",
    "text": "The Candidates\nLike most things in R there are a bunch of ways to complete the same task. The approaches below are a few that I could think of:\n\npaste: base function for concatenating strings\npaste0: base function for concatenating strings (paste(..., sep=\"\")\nsprintf: base function for C-style sprintf character formatting of strings\nstringi::stri_c: stringi function for combining multiple character vectors\nstringr::str_c: stringr function that wraps stringi but conforms to tidyverse recycling and NA rules\nglue::glue: String interpolation. Has to be converted to a vector after interpolation.\n\n\nlibrary(glue)\nlibrary(stringi)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(bench)"
  },
  {
    "objectID": "posts/joining-strings/index.html#create-some-test-strings",
    "href": "posts/joining-strings/index.html#create-some-test-strings",
    "title": "Joining Millions of Strings in R",
    "section": "Create some test strings",
    "text": "Create some test strings\nLet’s make 1 million “chromosome-start” strings to simulate a realistic test set size.\n\nset.seed(1234)\n\nN &lt;- 1e6\nchroms &lt;- sample(\n  paste0(\"chr\", as.character(1:22, \"X\", \"Y\")),\n  size = N, replace = TRUE\n)\nstarts &lt;- as.character(sample.int(1e5, size = N, replace = TRUE))"
  },
  {
    "objectID": "posts/joining-strings/index.html#time-it",
    "href": "posts/joining-strings/index.html#time-it",
    "title": "Joining Millions of Strings in R",
    "section": "Time it!",
    "text": "Time it!\nThe code below times the execution of each of the string joining expressions 30 times. Since glue has to be converted to a character vector after interpolation, I created a second expression without the coercion to see how much that affects the timing. It is for this reason that the check argument is set to FALSE.\n\n# Run the benchmark\nresults &lt;- bench::mark(\n  \"paste\" = paste(chroms, starts, sep = \"-\"),\n  \"paste0\" = paste0(chroms, \"-\", starts),\n  \"sprintf\" = sprintf(\"%s%s%s\", chroms, \"-\", starts),\n  \"stri_c\" = stri_c(chroms, starts, sep = \"-\"),\n  \"str_c\" = str_c(chroms, starts, sep = \"-\"),\n  \"glue\" = as.character(glue(\"{chroms}-{starts}\")),\n  \"glue2\" = glue(\"{chroms}-{starts}\"),\n  check = FALSE,\n  memory = TRUE,\n  min_time = Inf,\n  max_iterations = 30\n)\n\n# Plot the results\nautoplot(results) +\n  labs(\n    title = \"Execution Time for Joining 1 Million Element Vectors\",\n    x = \"Time\",\n    y = \"Function\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    legend.position = \"bottom\"\n  )\n\nLoading required namespace: tidyr\n\n\n\n\n\nIt looks like the clear winners are stringi::stri_c and stringr::str_c. This makes sense. What was surprising is that paste0 performs worse than paste with sep=\"-\".\nThere’s one, somewhat hacky, solution I wanted to test. What if we were to instead write the data out as a file where the vectors are concatenated with “-” as a delimiter and then read this concatenated file back in as a single column?"
  },
  {
    "objectID": "posts/joining-strings/index.html#data.tablefwrite-data.tablefread",
    "href": "posts/joining-strings/index.html#data.tablefwrite-data.tablefread",
    "title": "Joining Millions of Strings in R",
    "section": "data.table::fwrite & data.table::fread",
    "text": "data.table::fwrite & data.table::fread\nAssuming more threads makes this faster(?) I’ll max out all 8 cores on my machine. Also, I doubt that this specific example will be faster on only 1 million elements so I’ll bump up the number of elements to 100 million and see how it compares against the fastest function from above.\n\nlibrary(data.table)\nsetDTthreads(percent = 100)\n\ndt &lt;- data.table(\n  Chromosome = sample(paste0(\"chr\", as.character(1:22, \"X\", \"Y\")),\n    size = 1e8, replace = TRUE\n  ),\n  Start = as.character(sample.int(1e5, size = 1e8, replace = TRUE))\n)\n\nWrite the data out using fwrite(..., sep=\"-\") to concatenate the columns and then read them back in as a single concatenated column with fread\n\nsystem.time({\n  fwrite(dt, file = \"test.txt\", sep = \"-\", col.names = FALSE)\n  fread(\"test.txt\", col.names = \"chr_start\")\n})\n\n   user  system elapsed \n 57.580   1.586  34.265 \n\n\nAnd how does this compare to concatenating with stri_c\n\nsystem.time(dt[, chr_start := stri_c(Chromosome, Start, sep=\"-\")])\n\n   user  system elapsed \n 36.670   1.280  37.952 \n\n\nIt looks kinda crazy but maybe if you have enough space to spare and a lot of threads to throw at it then the ‘hacky’ solution might be a fast alternative for extremely large string concatenations; although the difference may not be large enough to to really matter too much in the end."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My place for rough ideas about bioinformatics, statistics, and coding."
  },
  {
    "objectID": "posts/bootstrap-differential-expression/index.html",
    "href": "posts/bootstrap-differential-expression/index.html",
    "title": "Bootstrapping Differential Expression",
    "section": "",
    "text": "I have been playing around with the idea of bootstrapping differential expression analyses, inspired by this excellent blog post from Professor Frank Harrell on how to do bad biomarker research. In the section titled, “Difficulties of picking ‘winners’” he writes:\n\nEfron’s bootstrap can be used to fully account for the difficulty of the biomarker selection task. Selection of winners involves computing some statistic for each candidate marker, and sorting features by these strength-of-association measures. The statistic can be a crude unadjusted measure (correlation coefficient or unadjusted odds ratio, for example), or an adjusted measure. For each of a few hundred samples with replacement from the original raw dataset, one repeats the entire analysis afresh for each re-sample. All the biomarker candidates are ranked by the chosen statistic, and bootstrap percentile confidence intervals for these ranks are computed over all re-samples. 0.95 confidence limits for the rank of each candidate marker capture the stability of the ranks.\n\nSince I do a lot of analyses that involve differential expression testing where samples are done in triplcate, I was curious if I could apply this resampling strategy to my work to get a better idea of how often my winners are really winners given we often have such small groups of samples.\nThis blog post is not about performing this bootstrapping workflow (I’ll save that for later). Rather, I want to explore how many unique bootstrap resamples we expect to generate given triplicate samples and how often we should expect any given pattern to by sampled."
  },
  {
    "objectID": "posts/bootstrap-differential-expression/index.html#das-boot",
    "href": "posts/bootstrap-differential-expression/index.html#das-boot",
    "title": "Bootstrapping Differential Expression",
    "section": "",
    "text": "I have been playing around with the idea of bootstrapping differential expression analyses, inspired by this excellent blog post from Professor Frank Harrell on how to do bad biomarker research. In the section titled, “Difficulties of picking ‘winners’” he writes:\n\nEfron’s bootstrap can be used to fully account for the difficulty of the biomarker selection task. Selection of winners involves computing some statistic for each candidate marker, and sorting features by these strength-of-association measures. The statistic can be a crude unadjusted measure (correlation coefficient or unadjusted odds ratio, for example), or an adjusted measure. For each of a few hundred samples with replacement from the original raw dataset, one repeats the entire analysis afresh for each re-sample. All the biomarker candidates are ranked by the chosen statistic, and bootstrap percentile confidence intervals for these ranks are computed over all re-samples. 0.95 confidence limits for the rank of each candidate marker capture the stability of the ranks.\n\nSince I do a lot of analyses that involve differential expression testing where samples are done in triplcate, I was curious if I could apply this resampling strategy to my work to get a better idea of how often my winners are really winners given we often have such small groups of samples.\nThis blog post is not about performing this bootstrapping workflow (I’ll save that for later). Rather, I want to explore how many unique bootstrap resamples we expect to generate given triplicate samples and how often we should expect any given pattern to by sampled."
  },
  {
    "objectID": "posts/bootstrap-differential-expression/index.html#how-many-possible-unique-combinations-of-the-data-are-there",
    "href": "posts/bootstrap-differential-expression/index.html#how-many-possible-unique-combinations-of-the-data-are-there",
    "title": "Bootstrapping Differential Expression",
    "section": "How many possible unique combinations of the data are there?",
    "text": "How many possible unique combinations of the data are there?\nSince a typical experiment consists of samples done in triplicate the question then becomes, how many unique ways of bootstrapping samples are there? The reason I care about unique resamples is because when estimating differential expression we are comparing the mean expression between two groups and therefore a resample consisting of [control1, control1, control2] will give the same mean as resampling [control2, control1, control1].\nSince I’m no good at math, to examine this question I’ll generate a grid of all possibilities and count up the unique combinations.\n\nlibrary(data.table)\n\n# Create all combinations of the three samples\nsamples &lt;- c(\"A\", \"B\", \"C\")\ndt &lt;- setDT(expand.grid(samples, samples, samples))\n\n# Combine into a single string representing the selected samples\ndt[, sample := paste0(Var1, Var2, Var3)]\n\n# Count up the number of letters represented in each string\ndt[, `:=`(N_A = stringr::str_count(sample, \"A\"),\n          N_B = stringr::str_count(sample, \"B\"),\n          N_C = stringr::str_count(sample, \"C\"))]\n\n# Count up the unique counts -- total number of rows gives the unique ways of\n#  generating bootstraps for triplicates\n(nrow(unique(dt[, .(N_A, N_B, N_C)])))\n\n[1] 10\n\n\nAs it turns out, this question has been asked and answered already and the theoretical answer is given by \\(2n-1\\choose{n}\\). So \\({2(3)-1\\choose{3}}={5\\choose3}=10\\)\nJust to be sure, let’s try again with 4 letters and check against the theoretical answer\n\nsamples2 &lt;- c(\"A\", \"B\", \"C\", \"D\")\ndt2 &lt;- setDT(expand.grid(samples2, samples2, samples2, samples2))\ndt2[, sample := paste0(Var1, Var2, Var3, Var4)]\ndt2[, `:=`(N_A = stringr::str_count(sample, \"A\"),\n           N_B = stringr::str_count(sample, \"B\"),\n           N_C = stringr::str_count(sample, \"C\"),\n           N_D = stringr::str_count(sample, \"D\"))]\n(nrow(unique(dt2[, .(N_A, N_B, N_C, N_D)])))\n\n[1] 35\n\n\n\nchoose(2*4-1, 4)\n\n[1] 35\n\n\nSo if I want to generate bootstrap resamples for a 3x3 experiment there should be 10 x 10 = 100 unique comparisons that I can make. But how often should we expect to see any given pattern in a set of triplicates if we perform a bunch of bootstraps?"
  },
  {
    "objectID": "posts/bootstrap-differential-expression/index.html#pattern-counts",
    "href": "posts/bootstrap-differential-expression/index.html#pattern-counts",
    "title": "Bootstrapping Differential Expression",
    "section": "Pattern counts",
    "text": "Pattern counts\nWhat is the expected proportion of each pattern in the triplicate experiment if we are to resample with replacement? We can find this by taking the proportions of each unique pattern from above.\n\n# Create a string from all of the unique ways to count samples\ndt[, patterns := paste0(N_A, N_B, N_C)]\n\n# Find the proportion of each of the possible ways to combine samples\nsort(table(dt$patterns) / sum(table(dt$patterns)))\n\n\n       003        030        300        012        021        102        120 \n0.03703704 0.03703704 0.03703704 0.11111111 0.11111111 0.11111111 0.11111111 \n       201        210        111 \n0.11111111 0.11111111 0.22222222 \n\n\nWe can see that some patterns are more likely than others. For example, we are just as likely to select 0 As, 0 Bs, and 3 Cs as we are 3 As, 0 Bs, and 0 Cs. This is interesting because it suggests that ~22% of our resamples should contain the original samples, ~66% should contain one duplicated sample and ~11% should contain triplicates of single sample.\nWe should see this if we generate samples and count the occurrences.\n\nset.seed(1011001)\n\nbootSamples &lt;- function() {\n  # Generate the random string of selected samples\n  s &lt;- paste(sample(c(\"A\", \"B\", \"C\"), replace = TRUE), collapse=\"\")\n  \n  # Count the number of times any individual occurs in the string\n  data.table(\n    N_A = stringr::str_count(s, \"A\"),\n    N_B = stringr::str_count(s, \"B\"),\n    N_C = stringr::str_count(s, \"C\")\n    )\n}\n\n# Generate 100 bootstrap resamples\nbootstraps &lt;- replicate(1e2, bootSamples(), simplify = FALSE)\nbootstraps &lt;- rbindlist(bootstraps)\nbootstraps[, patterns := paste0(N_A, N_B, N_C)]\n\n# Count the frequency of the observed patterns\nsort(table(bootstraps$patterns) / sum(table(bootstraps$patterns)))\n\n\n 300  030  003  021  210  120  012  102  201  111 \n0.02 0.04 0.05 0.09 0.09 0.10 0.11 0.13 0.17 0.20 \n\n\nGenerating 100 samples gets us close to the theoretical values and on average will converge on the theoretical values."
  },
  {
    "objectID": "posts/bootstrap-differential-expression/index.html#thoughts",
    "href": "posts/bootstrap-differential-expression/index.html#thoughts",
    "title": "Bootstrapping Differential Expression",
    "section": "Thoughts",
    "text": "Thoughts\nThis is pretty interesting since it suggests that about 5% of the time (0.22 * 0.22 = 0.048) when resampling two groups of triplicate samples I should expect to get back the same results as in the original analysis."
  }
]