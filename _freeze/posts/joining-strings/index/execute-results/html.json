{
  "hash": "e0296bc040cfd540a2be3c81e9a64a4a",
  "result": {
    "markdown": "---\ntitle: \"Joining Millions of Strings in R\"\ndescription: \"Is there a fast way to join millions of string in R?\"\nauthor: \"Gennaro Calendo\"\ndate: \"7/20/2023\"\ncategories: [R]\n---\n\n\nI recently had to concatenate tens of millions of strings into a single column of \nstrings (thanks Bismark coverage files...) to be used as a unique rownames for a \ncombined data.frame of methylation values for multiple samples. I was surprised \nwhen even my `data.table` code which was something like, \n`dt[, Location := paste(chr, start, sep = \"-\")]`, was taking minutes. \n\nSo then, what is the fastest way to combine a bunch of strings? \n\nSince I really care about this in the context of genetic data I'll simulate \nstrings from \"Chromosomes\" and \"Start positions\" and concatenate them into a \nsingle \"Loci\". e.g. \"chr1-45678\" \n\n## Load Libraries\n\nThe candidates that I want to test are:\n\n- `paste`: base function for concatenating strings\n- `paste0`: base function for concatenating strings (`paste(..., sep=\"\"`)\n- `sprintf`: base function for C-style `sprintf` character formatting of strings\n- `stringi::stri_c`: `stringi` function for combining multiple character vectors\n- `stringr::str_c`: `stringr` function that wraps `stringi` but conforms to \ntidyverse recycling and NA rules\n- `glue::glue`: String interpolation. Has to be converted to a vector after\ninterpolation. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glue)\nlibrary(stringi)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(bench)\n```\n:::\n\n\n## Create some test strings\n\nLet's make 1 million \"chromosome-start\" strings to simulate a realistic test \nset size.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nN <- 1e6\nchroms <- sample(paste0(\"chr\", as.character(1:22, \"X\", \"Y\")), size = N, replace = TRUE)\nstarts <- as.character(sample.int(1e5, size = N, replace = TRUE))\n```\n:::\n\n\n## Time it!\n\nThe code below times the execution of each of the string joining expressions\n30 times. Since `glue` has to be converted to a character vector after \ninterpolation, I created a second expression without the coercion to see how \nmuch that affects the timing. It is for this reason that the `check` argument \nis set to FALSE.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run the benchmark\nresults <- bench::mark(\n  \"paste\" = paste(chroms, starts, sep = \"-\"),\n  \"paste0\" = paste0(chroms, \"-\", starts),\n  \"sprintf\" = sprintf(\"%s%s%s\", chroms, \"-\", starts),\n  \"stri_c\" = stri_c(chroms, starts, sep = \"-\"),\n  \"str_c\" = str_c(chroms, starts, sep = \"-\"),\n  \"glue\" = as.character(glue(\"{chroms}-{starts}\")),\n  \"glue2\" = glue(\"{chroms}-{starts}\"),\n  check = FALSE,\n  memory = TRUE,\n  min_time = Inf,\n  max_iterations = 30\n)\n\n# Plot the results\nautoplot(results) +\n  labs(title = \"Execution Time for Joining 1 Million Element Vectors\",\n       x = \"Time\",\n       y = \"Function\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 16),\n        legend.position = \"bottom\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required namespace: tidyr\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nIt looks like the clear winners are `stringi::stri_c` and `stringr::str_c`. \nThis makes sense. What was surprising is that `paste0` performs worse than \n`paste` with `sep=\"-\"`. \n\nThere's one, somewhat hacky, solution I wanted to test. What if we were to \ninstead write the data out as a file where the vectors are concatenated with \n\"-\" as a delimiter and then read this concatenated file back in as a single \ncolumn?\n\n## `data.table::fwrite` & `data.table::fread`\n\nAssuming more threads makes this faster(?) I'll max out all 8 cores on my \nmachine. Also, I doubt that this specific example will be faster on only \n1 million elements so I'll bump up the number of elements to 100 million\nand see how it compares against the fastest function from above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\nsetDTthreads(percent = 100)\n\ndt <- data.table(\n  Chromosome = sample(paste0(\"chr\", as.character(1:22, \"X\", \"Y\")), \n                      size = 1e8, replace = TRUE),\n  Start = as.character(sample.int(1e5, size = 1e8, replace = TRUE))\n)\n```\n:::\n\n\nWrite the data out using `fwrite(..., sep=\"-\")` to concatenate the columns\nand then read them back in as a single concatenated column with `fread`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time({\n  fwrite(dt, file = \"test.txt\", sep = \"-\", col.names = FALSE)\n  fread(\"test.txt\", col.names = \"chr_start\")\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 58.486   1.785  38.855 \n```\n:::\n:::\n\n\nAnd how does this compare to concatenating with `stri_c`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time(dt[, chr_start := stri_c(Chromosome, Start, sep=\"-\")])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 36.527   1.260  37.797 \n```\n:::\n:::\n\n\nIt looks kinda crazy but maybe if you have enough space to spare and a lot of\nthreads to throw at it then the 'hacky' solution might be a fast alternative for \nextremely large string concatenations; although the difference may not be large \nenough to to really matter too much in the end. \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}